{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e231f982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Is Aaronson guilty?\n",
      "A: Yes, according to the provided context, Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with. The protagonist remembers this as he thinks, \"Jones, Aaronson, and Rutherford were guilty of the crimes they were charged with.\"\n",
      "\n",
      "\n",
      "Q: What message did he write in the table?\n",
      "A: He wrote: \"2+2=5\"\n",
      "\n",
      "\n",
      "Q: Who is Julia?\n",
      "A: Julia is a person who Winston loves and cares for deeply. He cries out for her in a moment of weakness and later wishes for her to be punished instead of him.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# RAG (Stuff Documents 체인, 수동 구현) + ConversationBufferMemory + 로컬 파일 + 캐시 임베딩\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ----- 배운 임포트 우선 -----\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.storage import LocalFileStore\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough, RunnableLambda\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# StuffDocumentsChain (구 버전 경로)\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# ===== 0) 환경설정 (.env) =====\n",
    "# .env 파일에 OPENAI_API_KEY 등 중요 키 저장해두셨다고 하셔서 로드합니다.\n",
    "# 예) OPENAI_API_KEY=sk-xxx\n",
    "load_dotenv()\n",
    "assert os.getenv(\"OPENAI_API_KEY\"), \"OPENAI_API_KEY가 .env에 설정되어 있어야 합니다.\"\n",
    "\n",
    "# ===== 1) LLM & 캐시 디렉토리 =====\n",
    "llm = ChatOpenAI(\n",
    "    temperature=0.1,  # 안정적 응답\n",
    ")\n",
    "\n",
    "cache_dir = LocalFileStore(\"./.cache/\")  # 요청사항: 그대로 사용\n",
    "\n",
    "# ===== 2) 로컬 문서 로딩 & 청킹 =====\n",
    "# 요청사항: 웹이 아니라 로컬 파일 경로로 로드\n",
    "loader = UnstructuredFileLoader(\"/Users/yy/Desktop/study/full-gpt/files/document.txt\")\n",
    "\n",
    "splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "docs = loader.load_and_split(text_splitter=splitter)\n",
    "\n",
    "# ===== 3) 임베딩 + 캐시 임베딩 + 벡터스토어 =====\n",
    "base_embeddings = OpenAIEmbeddings()\n",
    "cached_embeddings = CacheBackedEmbeddings.from_bytes_store(base_embeddings, cache_dir)\n",
    "vectorstore = FAISS.from_documents(docs, cached_embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ===== 4) Stuff Documents 체인 (수동 구현) =====\n",
    "# 메모리와 함께 사용할 수 있도록 chat_history를 프롬프트에 포함합니다.\n",
    "stuff_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a careful reading assistant. Use ONLY the provided context to answer. \"\n",
    "            \"If the answer is not in the context, say you don't know.\"\n",
    "        ),\n",
    "        # 메모리에서 들어오는 대화 이력을 받을 placeholder\n",
    "        (\"system\", \"Conversation so far:\\n{chat_history}\"),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Question: {question}\\n\\n\"\n",
    "            \"Use these documents as context:\\n{context}\\n\\n\"\n",
    "            \"Answer concisely, quote exact phrases when relevant.\"\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# LLMChain + StuffDocumentsChain 조합으로 \"수동\" Stuff 체인 구성\n",
    "llm_chain = LLMChain(llm=llm, prompt=stuff_prompt)\n",
    "stuff_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",  # 프롬프트의 {context}와 연결\n",
    ")\n",
    "\n",
    "# ===== 5) ConversationBufferMemory 부여 (수동 연결) =====\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=False,  # 문자열로 합쳐서 프롬프트에 넣고 싶으면 False\n",
    ")\n",
    "\n",
    "# ===== 6) RAG 파이프라인 (수동 결합) =====\n",
    "def retrieve_then_stuff(question: str) -> str:\n",
    "    \"\"\"\n",
    "    1) retriever로 문서 검색\n",
    "    2) 메모리에서 chat_history를 불러와 prompt 채우기\n",
    "    3) StuffDocumentsChain 실행\n",
    "    4) 메모리에 Q/A 저장\n",
    "    \"\"\"\n",
    "    # 검색\n",
    "    rel_docs = retriever.get_relevant_documents(question)\n",
    "\n",
    "    # 메모리에서 현재까지의 대화 이력을 문자열로 로드\n",
    "    mem_vars = memory.load_memory_variables({})\n",
    "    chat_history = mem_vars.get(\"chat_history\", \"\")\n",
    "\n",
    "    # StuffDocumentsChain 실행\n",
    "    answer = stuff_chain.run(\n",
    "        {\n",
    "            \"question\": question,\n",
    "            \"chat_history\": chat_history,\n",
    "            \"input_documents\": rel_docs,  # StuffDocumentsChain은 이 키로 문서 받음\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # 메모리 업데이트\n",
    "    memory.save_context({\"question\": question}, {\"answer\": answer})\n",
    "    return answer\n",
    "\n",
    "# ===== 7) 과제의 3개 질문 실행 =====\n",
    "questions = [\n",
    "    \"Is Aaronson guilty?\",\n",
    "    \"What message did he write in the table?\",\n",
    "    \"Who is Julia?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    a = retrieve_then_stuff(q)\n",
    "    print(f\"\\nQ: {q}\\nA: {a}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
